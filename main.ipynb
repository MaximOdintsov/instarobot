{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import random\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from helpers.selenium_management import start_driver, open_link, get_wait_element, get_wait_elements, close_driver, get_link_elements, get_links\n",
    "from helpers.utils import extract_original_link, parse_activity_data, validate_instagram_url #, validate_and_extract_domain\n",
    "from helpers.excel import PyXLWriter\n",
    "\n",
    "import config\n",
    "from robot import auth, account_get_post_links, account_send_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import re\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from ml_models.custom_transformers import TextCleaner, DomainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataFrame\n",
    "file_path = 'data/modеls/train_data/merged_table.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Шаг 1: Предобработка текста\n",
    "##################################\n",
    "\n",
    "# Создание DataFrame\n",
    "file_path = 'data/modеls/train_data/merged_table.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Замена пустых значений\n",
    "df.fillna(\n",
    "    {   \n",
    "        'Предсказанный тип аккаунта': 'OTHER',\n",
    "        'Описание страницы': '',\n",
    "        'Ссылки из описания': '', \n",
    "        'Ссылки из контактов': '',\n",
    "        'Кол-во постов': 0\n",
    "    }, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# 1 шаг: Замена имени столбца\n",
    "# 2 шаг: Удаление всех строк с типом \"OTHER\"\n",
    "df.rename(columns={\"Предсказанный тип аккаунта\": \"Тип аккаунта\"}, inplace=True)\n",
    "df = df[df[\"Тип аккаунта\"] != \"OTHER\"]\n",
    "\n",
    "# Заменяем значения столбца на 1, если не NaN, и на 0, если NaN\n",
    "# 1 шаг - замена на True и False\n",
    "# 2 шаг - замена на 1 и 0\n",
    "df['Почта существует'] = df['Найденная почта'].notna().astype(int)\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 2: Делим на X и y + разделяем данные на обучающую и тестовую выборки\n",
    "##################################\n",
    "X = df[[\n",
    "    \"Описание страницы\",  # text\n",
    "    \"Ссылки из описания\",  # text\n",
    "    \"Ссылки из контактов\",  # text\n",
    "    \"Почта существует\",  # 1/0\n",
    "    \"Кол-во постов\",  # num\n",
    "]]\n",
    "y = df[\"Тип аккаунта\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "##################################\n",
    "# Шаг 3: Настраиваем векторизацию и классификацию + cобираем Pipeline \n",
    "##################################\n",
    "\n",
    "# Создание ColumnTransformer для обработки каждого столбца\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            'desc_tfidf',  # Для текстовых значений \n",
    "            Pipeline([\n",
    "                ('text_cleaner', TextCleaner()),\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english',  # Стоп-слова\n",
    "                                          ngram_range=(1, 3),  # Использование триграмм\n",
    "                                          max_features=1250,  # Увеличение числа признаков\n",
    "                                          sublinear_tf=True)  # Использование логарифмического масштаба для частоты терминов\n",
    "                ),\n",
    "            ]),\n",
    "            'Описание страницы'\n",
    "            \n",
    "        ),\n",
    "        (\n",
    "            'desc_links_tfidf', \n",
    "            Pipeline([\n",
    "                ('extract_domains', DomainExtractor()),  # Кастомный экстрактор для ссылок\n",
    "                ('tfidf', TfidfVectorizer())  # Для текстовых значений\n",
    "            ]), \n",
    "            'Ссылки из описания'\n",
    "        ),\n",
    "        (\n",
    "            'contact_links_tfidf', \n",
    "            Pipeline([\n",
    "                ('extract_domains', DomainExtractor()),  # Кастомный экстрактор для ссылок\n",
    "                ('tfidf', TfidfVectorizer())  # Для текстовых значений\n",
    "            ]),\n",
    "            'Ссылки из контактов'\n",
    "        ),\n",
    "        (\n",
    "            'binary', \n",
    "            'passthrough',  # Пропускает значения без изменений\n",
    "            ['Почта существует']\n",
    "        ),\n",
    "        (\n",
    "            'num_scaler', \n",
    "            StandardScaler(),  # Для числовых значений\n",
    "            ['Кол-во постов']\n",
    "        ),\n",
    "    ],\n",
    "    remainder='drop'  # Удаляет остальные столбцы, если они есть\n",
    ")\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 4: Собираем Pipeline\n",
    "##################################\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "# Использование методов ресэмплинга\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),  # Увеличение числа примеров для редких классов с помощью методов, таких как SMOTE.\n",
    "    ('clf', LogisticRegression(class_weight='balanced', max_iter=1000))  # 0.77\n",
    "])\n",
    "\n",
    "# ##################################\n",
    "# # Шаг 5: Обучение и оценка\n",
    "# ##################################\n",
    "\n",
    "# Обучаем\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Смотрим отчёт\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в файле data/modеls/account_type.pkl\n",
      "Модель загружена из файла data/modеls/account_type.pkl\n",
      "Предсказанные классы: ['BEATMAKER']\n",
      "Вероятности классов: [[0.03556533 0.88062202 0.01906485 0.04723903 0.01750876]]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "##################################\n",
    "# Шаг 6: Обучение модели на всех имеющихся данных\n",
    "##################################\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "##################################\n",
    "# Шаг 7: Сохранение модели\n",
    "##################################\n",
    "\n",
    "model_path = 'data/modеls/account_type.pkl'\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"Модель сохранена в файле {model_path}\")\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 8: Загрузка модели\n",
    "##################################\n",
    "\n",
    "loaded_model = joblib.load(model_path)\n",
    "print(f\"Модель загружена из файла {model_path}\")\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 9: Использование модели\n",
    "##################################\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'Описание страницы': [\n",
    "        '''Colt Blumenthal\n",
    "mixedbycolt\n",
    "🚀 #1 Billboard Engineer, 💿 Multi-Platinum\n",
    "💻Engineered for 100+ Major Artists\n",
    "🏅 SAE Institute Alumni Hall of Fame\n",
    "🔊Premium Mixing & Mastering Service\n",
    "linktr.ee/mixedbycolt'''\n",
    "    ],\n",
    "    'Ссылки из описания': [\n",
    "        '''https://linktr.ee/mixedbycolt?fbclid=PAZXh0bgNhZW0CMTEAAaYAv9ZUzP17l1o8wVsSVjvTAzUJT9CppNIKLuP0FENyTVKEOAH3OmmBnCA_aem_FRFJBiIdaHkZ9aYHEaCLHQ\n",
    "https://www.instagram.com/explore/tags/1/\n",
    "https://www.threads.net/@mixedbycolt?xmt=AQGzUA43HPJ5PxWfBGKXf29C7EUpu6r6IVFcdtq1jbBJa8Y'''\n",
    "    ],\n",
    "    'Ссылки из контактов': [\n",
    "        ''\n",
    "        # 'https://soundcloud.com/playlist/ddw31e\\nhttps://youtube.com/watch?v=12345'\n",
    "    ],\n",
    "    'Почта существует': [0],\n",
    "    'Кол-во постов': [90]\n",
    "})\n",
    "\n",
    "preds = loaded_model.predict(new_data)\n",
    "print(\"Предсказанные классы:\", preds)\n",
    "\n",
    "proba = loaded_model.predict_proba(new_data)\n",
    "print(\"Вероятности классов:\", proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BEATMAKER'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип аккаунта\n",
      "ARTIST       358\n",
      "COMMUNITY    179\n",
      "MARKET        83\n",
      "LABEL         37\n",
      "BEATMAKER     19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Подсчет каждого типа аккаунта\n",
    "print(df['Тип аккаунта'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision (Точность) - какая доля действительно относится к этому классу?\n",
    "# recall (Полнота) - какая доля была предсказана как этот класс?\n",
    "# f1-score (Среднее гармоническое между точностью и полнотой)\n",
    "# support (Число реальных образцов)\n",
    "\n",
    "# accuracy: Общая доля правильных предсказаний по всем классам.\n",
    "\n",
    "# avg: Средние значения precision, recall и f1-score по всем классам (без учёта их пропорций). \n",
    "# То есть каждый класс “взвешен” одинаково, даже если в выборке один класс встречается гораздо чаще другого.\n",
    "\n",
    "# weighted avg: Взвешенное среднее тех же метрик, где вес — это support (количество объектов каждого класса). \n",
    "# Таким образом, частые классы влияют на метрики сильнее, чем редкие.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтруем только строки, где \"Предсказанный тип аккаунта\" == \"ARTIST\"\n",
    "df_artist = df[df[\"Предсказанный тип аккаунта\"] == \"ARTIST\"]\n",
    "\n",
    "# Считаем самые популярные хэштеги среди этих строк\n",
    "hashtags_count = df_artist[\"Хэштег, по которому найден аккаунт\"].value_counts()\n",
    "\n",
    "# Смотрим результат (топ 10, например)\n",
    "print(hashtags_count.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_mask = df.duplicated()\n",
    "print(\"Число дубликатов:\", duplicates_mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединение данных с обработанными таблицами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from helpers.excel import write_excel\n",
    "\n",
    "from database.orm import async_session, get_all_accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Папка с файлами .ods\n",
    "folder_path = 'data/modеls/tables'\n",
    "\n",
    "# Получаем список файлов .ods в папке\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.ods')]\n",
    "\n",
    "# Создаем пустой DataFrame для объединенных данных\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    # Загружаем данные из каждого файла\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    data = pd.read_excel(file_path, engine='odf')  # Используем 'odf' для .ods файлов\n",
    "    \n",
    "    # Очистка данных\n",
    "    data['Тип аккаунта (записывается всегда). Значения: ARTIST, BEATMAKER, LABEL, MARKET, COMMUNITY, OTHER'] = data['Тип аккаунта (записывается всегда). Значения: ARTIST, BEATMAKER, LABEL, MARKET, COMMUNITY, OTHER'].replace(np.nan, 'OTHER')\n",
    "\n",
    "    # Объединяем данные по названию столбцов\n",
    "    combined_df = pd.concat([combined_df, data], ignore_index=True)\n",
    "\n",
    "async def update_account_types(async_session, data: pd.DataFrame):\n",
    "    async with async_session() as session:\n",
    "        # Получаем все аккаунты из базы данных\n",
    "        accounts = await get_all_accounts(async_session)\n",
    "\n",
    "        # Создаем словарь для быстрого поиска аккаунтов по ссылке\n",
    "        account_dict = {account.link: account for account in accounts}\n",
    "\n",
    "        # Обновляем типы аккаунтов на основе данных из DataFrame\n",
    "        for _, row in data.iterrows():\n",
    "            link = row.get('Ссылка на аккаунт').strip()\n",
    "            account_type = row.get('Тип аккаунта (записывается всегда). Значения: ARTIST, BEATMAKER, LABEL, MARKET, COMMUNITY, OTHER').strip()\n",
    "            # print(f'link: {link}')\n",
    "            # print(f'account_type: {account_type}')\n",
    "\n",
    "            if link in account_dict and account_type:\n",
    "                account = account_dict[link]\n",
    "                account.account_type = account_type\n",
    "                # print(f'account: {account.link}')\n",
    "                # print(f'account_type: {account.account_type}')\n",
    "                session.add(account)\n",
    "        # Сохраняем изменения в базе данных\n",
    "        await session.commit()\n",
    "        \n",
    "await update_account_types(async_session, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = await get_all_accounts(async_session)\n",
    "write_excel(accounts, out_path='data/modеls/row_data/merged_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
