{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание робота selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from robot.helpers.selenium_management import open_link, get_wait_element, get_wait_elements, get_link_elements, get_links, start_driver, close_driver\n",
    "\n",
    "from robot.conf import config\n",
    "from robot.helpers.utils import (\n",
    "    extract_emails,\n",
    "    validate_instagram_url,\n",
    "    POST_VALUE,\n",
    "    ACCOUNT_VALUE\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"]\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = \"1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms\"\n",
    "SAMPLE_RANGE_NAME = \"Class Data!A2:E\"\n",
    "\n",
    "\n",
    "def main():\n",
    "  \"\"\"Shows basic usage of the Sheets API.\n",
    "  Prints values from a sample spreadsheet.\n",
    "  \"\"\"\n",
    "  creds = None\n",
    "  # The file token.json stores the user's access and refresh tokens, and is\n",
    "  # created automatically when the authorization flow completes for the first\n",
    "  # time.\n",
    "  if os.path.exists(\"token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "  # If there are no (valid) credentials available, let the user log in.\n",
    "  if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "      creds.refresh(Request())\n",
    "    else:\n",
    "      flow = InstalledAppFlow.from_client_secrets_file(\n",
    "          \"credentials.json\", SCOPES\n",
    "      )\n",
    "      creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"token.json\", \"w\") as token:\n",
    "      token.write(creds.to_json())\n",
    "\n",
    "  try:\n",
    "    service = build(\"sheets\", \"v4\", credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    sheet = service.spreadsheets()\n",
    "    result = (\n",
    "        sheet.values()\n",
    "        .get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=SAMPLE_RANGE_NAME)\n",
    "        .execute()\n",
    "    )\n",
    "    values = result.get(\"values\", [])\n",
    "\n",
    "    if not values:\n",
    "      print(\"No data found.\")\n",
    "      return\n",
    "\n",
    "    print(\"Name, Major:\")\n",
    "    for row in values:\n",
    "      # Print columns A and E, which correspond to indices 0 and 4.\n",
    "      print(f\"{row[0]}, {row[4]}\")\n",
    "  except HttpError as err:\n",
    "    print(err)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from robot.ml.custom_transformers import TextCleaner, DomainBinarizer\n",
    "from robot.models import AccountType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ARTIST       0.89      0.92      0.90        36\n",
      "   BEATMAKER       1.00      0.50      0.67         2\n",
      "   COMMUNITY       0.78      0.95      0.86        19\n",
      "       LABEL       1.00      0.60      0.75         5\n",
      "      MARKET       1.00      0.67      0.80         6\n",
      "\n",
      "    accuracy                           0.87        68\n",
      "   macro avg       0.93      0.73      0.80        68\n",
      "weighted avg       0.88      0.87      0.86        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# Шаг 1: Предобработка текста\n",
    "##################################\n",
    "\n",
    "# Создание DataFrame\n",
    "file_path = 'data/modеls/train_data/merged_table.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Замена пустых значений\n",
    "df.fillna(\n",
    "    {   \n",
    "        'Предсказанный тип аккаунта': 'OTHER',\n",
    "        'Описание страницы': '',\n",
    "        'Ссылки из описания': '', \n",
    "        'Ссылки из контактов': '',\n",
    "        'Кол-во постов': 0\n",
    "    }, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# 1 шаг: Замена имени столбца\n",
    "# 2 шаг: Удаление всех строк с типом \"OTHER\"\n",
    "df.rename(columns={\"Предсказанный тип аккаунта\": \"Тип аккаунта\"}, inplace=True)\n",
    "# df = df[df[\"Тип аккаунта\"] != \"OTHER\"]\n",
    "\n",
    "# Заменяем значения столбца на 1, если не NaN, и на 0, если NaN\n",
    "# 1 шаг - замена на True и False\n",
    "# 2 шаг - замена на 1 и 0\n",
    "df['Почта существует'] = df['Найденная почта'].notna().astype(int)\n",
    "\n",
    "# Разделяем ссылки в строках через '\\n'\n",
    "df['Ссылки из описания'] = df['Ссылки из описания'].str.split('\\n')\n",
    "df['Ссылки из контактов'] = df['Ссылки из контактов'].str.split('\\n')\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 2: Делим на X и y + разделяем данные на обучающую и тестовую выборки\n",
    "##################################\n",
    "X = df[[\n",
    "    \"Описание страницы\",  # text\n",
    "    \"Ссылки из описания\",  # text\n",
    "    \"Ссылки из контактов\",  # text\n",
    "    \"Почта существует\",  # 1/0\n",
    "    \"Кол-во постов\",  # num\n",
    "]]\n",
    "y = df[\"Тип аккаунта\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "##################################\n",
    "# Шаг 3: Настраиваем векторизацию и классификацию + cобираем Pipeline \n",
    "##################################\n",
    "\n",
    "# Создание ColumnTransformer для обработки каждого столбца\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            'desc_tfidf',  # Для текстовых значений \n",
    "            Pipeline([\n",
    "                ('text_cleaner', TextCleaner()),\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english',  # Стоп-слова\n",
    "                                          ngram_range=(1, 3),  # Использование триграмм\n",
    "                                          max_features=3500,  # Увеличение числа признаков\n",
    "                                          sublinear_tf=True)  # Использование логарифмического масштаба для частоты терминов\n",
    "                ),\n",
    "            ]),\n",
    "            'Описание страницы'\n",
    "            \n",
    "        ),\n",
    "        (\n",
    "            'desc_links_binarizer',\n",
    "            Pipeline([\n",
    "                ('binarizer', DomainBinarizer())\n",
    "                # при желании можно добавить StandardScaler() —\n",
    "                # но для бинарных фич это обычно не критично\n",
    "            ]),\n",
    "            'Ссылки из описания'\n",
    "        ),\n",
    "        (\n",
    "            'contact_links_binarizer',\n",
    "            Pipeline([\n",
    "                ('binarizer', DomainBinarizer())\n",
    "            ]),\n",
    "            'Ссылки из контактов'\n",
    "        ),\n",
    "        (\n",
    "            'binary', \n",
    "            'passthrough',  # Пропускает значения без изменений\n",
    "            ['Почта существует']\n",
    "        ),\n",
    "        (\n",
    "            'num_scaler', \n",
    "            StandardScaler(),  # Для числовых значений\n",
    "            ['Кол-во постов']\n",
    "        ),\n",
    "    ],\n",
    "    remainder='drop'  # Удаляет остальные столбцы, если они есть\n",
    ")\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 4: Собираем Pipeline\n",
    "##################################\n",
    "\n",
    "# Использование методов ресэмплинга\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),  # Увеличение числа примеров для редких классов с помощью методов, таких как SMOTE.\n",
    "    ('clf', LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
    "])\n",
    "\n",
    "# ##################################\n",
    "# # Шаг 5: Обучение и оценка\n",
    "# ##################################\n",
    "\n",
    "# Обучаем\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Смотрим отчёт\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в файле data/modеls/account_type.pkl\n",
      "Модель загружена из файла data/modеls/account_type.pkl\n",
      "Предсказанные классы: [<AccountType.BEATMAKER: 'BEATMAKER'>]\n",
      "Вероятности классов: [[0.06444823 0.85820526 0.02759148 0.03312428 0.01663076]]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "##################################\n",
    "# Шаг 6: Обучение модели на всех имеющихся данных\n",
    "##################################\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "##################################\n",
    "# Шаг 7: Сохранение модели\n",
    "##################################\n",
    "\n",
    "model_path = 'data/modеls/account_type.pkl'\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"Модель сохранена в файле {model_path}\")\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 8: Загрузка модели\n",
    "##################################\n",
    "\n",
    "loaded_model = joblib.load(model_path)\n",
    "print(f\"Модель загружена из файла {model_path}\")\n",
    "\n",
    "\n",
    "##################################\n",
    "# Шаг 9: Использование модели\n",
    "##################################\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'Описание страницы': [\n",
    "        '''Colt Blumenthal\n",
    "mixedbycolt\n",
    "🚀 #1 Billboard Engineer, 💿 Multi-Platinum\n",
    "💻Engineered for 100+ Major Artists\n",
    "🏅 SAE Institute Alumni Hall of Fame\n",
    "🔊Premium Mixing & Mastering Service\n",
    "linktr.ee/mixedbycolt'''\n",
    "    ],\n",
    "    'Ссылки из описания': [\n",
    "        [\n",
    "            'https://linktr.ee/mixedbycolt?fbclid=PAZXh0bgNhZW0CMTEAAaYAv9ZUzP17l1o8wVsSVjvTAzUJT9CppNIKLuP0FENyTVKEOAH3OmmBnCA_aem_FRFJBiIdaHkZ9aYHEaCLHQ',\n",
    "            'https://www.instagram.com/explore/tags/1/',\n",
    "            'https://www.threads.net/@mixedbycolt?xmt=AQGzUA43HPJ5PxWfBGKXf29C7EUpu6r6IVFcdtq1jbBJa8Y'\n",
    "        ]\n",
    "    ],\n",
    "    'Ссылки из контактов': [\n",
    "        ''\n",
    "        # 'https://soundcloud.com/playlist/ddw31e\\nhttps://youtube.com/watch?v=12345'\n",
    "    ],\n",
    "    'Почта существует': [0],\n",
    "    'Кол-во постов': [90]\n",
    "})\n",
    "\n",
    "preds = loaded_model.predict(new_data)\n",
    "preds_enum = [AccountType(s) for s in preds]  # [AccountType.ARTIST, AccountType.LABEL, ...]\n",
    "\n",
    "print(\"Предсказанные классы:\", preds_enum)\n",
    "\n",
    "proba = loaded_model.predict_proba(new_data)\n",
    "print(\"Вероятности классов:\", proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчет каждого типа аккаунта\n",
    "print(df['Тип аккаунта'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтруем только строки, где \"Предсказанный тип аккаунта\" == \"ARTIST\"\n",
    "df_artist = df[df[\"Предсказанный тип аккаунта\"] == \"ARTIST\"]\n",
    "\n",
    "# Считаем самые популярные хэштеги среди этих строк\n",
    "hashtags_count = df_artist[\"Хэштег, по которому найден аккаунт\"].value_counts()\n",
    "\n",
    "# Смотрим результат (топ 10, например)\n",
    "print(hashtags_count.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_mask = df.duplicated()\n",
    "print(\"Число дубликатов:\", duplicates_mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединение данных с обработанными таблицами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from helpers.excel import write_excel\n",
    "\n",
    "from database.orm import async_session, get_all_accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Папка с файлами .ods\n",
    "folder_path = 'data/modеls/tables'\n",
    "\n",
    "# Получаем список файлов .ods в папке\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.ods')]\n",
    "\n",
    "# Создаем пустой DataFrame для объединенных данных\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    # Загружаем данные из каждого файла\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    data = pd.read_excel(file_path, engine='odf')  # Используем 'odf' для .ods файлов\n",
    "    \n",
    "    # Очистка данных\n",
    "    data['Тип аккаунта (записывается всегда). Значения: ARTIST, BEATMAKER, LABEL, MARKET, COMMUNITY, OTHER'] = data['Тип аккаунта (записывается всегда). Значения: ARTIST, BEATMAKER, LABEL, MARKET, COMMUNITY, OTHER'].replace(np.nan, 'OTHER')\n",
    "\n",
    "    # Объединяем данные по названию столбцов\n",
    "    combined_df = pd.concat([combined_df, data], ignore_index=True)\n",
    "\n",
    "async def update_account_types(async_session, data: pd.DataFrame):\n",
    "    async with async_session() as session:\n",
    "        # Получаем все аккаунты из базы данных\n",
    "        accounts = await get_all_accounts(async_session)\n",
    "\n",
    "        # Создаем словарь для быстрого поиска аккаунтов по ссылке\n",
    "        account_dict = {account.link: account for account in accounts}\n",
    "\n",
    "        # Обновляем типы аккаунтов на основе данных из DataFrame\n",
    "        for _, row in data.iterrows():\n",
    "            link = row.get('Ссылка на аккаунт').strip()\n",
    "            account_type = row.get('Тип аккаунта (записывается всегда). Значения: ARTIST, BEATMAKER, LABEL, MARKET, COMMUNITY, OTHER').strip()\n",
    "            # print(f'link: {link}')\n",
    "            # print(f'account_type: {account_type}')\n",
    "\n",
    "            if link in account_dict and account_type:\n",
    "                account = account_dict[link]\n",
    "                account.account_type = account_type\n",
    "                # print(f'account: {account.link}')\n",
    "                # print(f'account_type: {account.account_type}')\n",
    "                session.add(account)\n",
    "        # Сохраняем изменения в базе данных\n",
    "        await session.commit()\n",
    "        \n",
    "await update_account_types(async_session, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = await get_all_accounts(async_session)\n",
    "write_excel(accounts, out_path='data/modеls/row_data/merged_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
